{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Cifar 100.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JmJwvKN5VE6"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from keras import models\r\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\r\n",
        "import numpy as np\r\n",
        "from keras.datasets import cifar100\r\n",
        "from keras.applications import MobileNet\r\n",
        "from keras.utils import np_utils\r\n",
        "from keras.layers import Dense,GlobalAveragePooling2D\r\n",
        "from keras.models import Model\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras import optimizers\r\n",
        "from keras.utils import to_categorical\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.applications import vgg16\r\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D,BatchNormalization\r\n",
        "import math\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from statistics import mean\r\n",
        "import keras\r\n",
        "import pandas as pd\r\n",
        "from sklearn.preprocessing import MaxAbsScaler\r\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1WmvJId5eWw"
      },
      "source": [
        "def encode(train_label,test_label):\r\n",
        "  train_label= to_categorical(train_label, 100)\r\n",
        "  test_label=to_categorical(test_label, 100)\r\n",
        "  return train_label,test_label"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGLkYkLM5j3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ff1495-6aa0-46da-9804-c3cc24c9b145"
      },
      "source": [
        "VGGmodel = VGG16()\r\n",
        "(train_features,train_labels),(test_features,test_labels) = cifar100.load_data()\r\n",
        "Features = np.concatenate((train_features,test_features))\r\n",
        "Labels = np.concatenate((train_labels,test_labels))\r\n",
        "\r\n",
        "shuffled_train_features,shuffled_test_features,shuffled_train_labels,shuffled_test_labels=train_test_split(Features,Labels, test_size=10000,random_state=42)\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 14s 0us/step\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IszP6r3E5nID",
        "outputId": "d273c218-d034-4569-f140-ba99ed213589"
      },
      "source": [
        "train_label,test_label = encode(shuffled_train_labels,shuffled_test_labels)\r\n",
        "test_label.shape\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1DpcQD25sXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bdfff4c-043f-4df3-cc5c-9ba6a1a4c2b9"
      },
      "source": [
        "#creating Model with bdropout of 0.4 nodes after every dense layer\r\n",
        "\r\n",
        "base_model = VGG16(weights='imagenet',include_top=False,input_shape=(32, 32, 3))\r\n",
        "output_model=Model(base_model.input,base_model.get_layer('block5_pool').output)\r\n",
        "\r\n",
        "preprocess_images_training =preprocess_input(shuffled_train_features)\r\n",
        "training_featureset=output_model.predict(preprocess_images_training)\r\n",
        "training_featureset=training_featureset.reshape(50000,512)\r\n",
        "\r\n",
        "preprocess_images_testing =preprocess_input(shuffled_test_features)\r\n",
        "testing_featureset=base_model.predict(preprocess_images_testing)\r\n",
        "testing_featureset=testing_featureset.reshape(10000,512)\r\n",
        "\r\n",
        "scale=MaxAbsScaler()\r\n",
        "training_featureset=scale.fit_transform(training_featureset)\r\n",
        "testing_featureset=scale.fit_transform(testing_featureset)\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOuAYtGP5vQ5"
      },
      "source": [
        "np.savetxt('Train_Features.txt',training_featureset,fmt='%.2f');\r\n",
        "np.savetxt('Train_labels.txt',train_label,fmt='%.2f');\r\n",
        "np.savetxt('Testing_features.txt',testing_featureset,fmt='%.2f')\r\n",
        "np.savetxt('Testing_labels.txt',test_label,fmt='%.2f')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNRpwnwDO7VQ",
        "outputId": "796c8a90-92c7-417b-f64b-9c5b0178a5fb"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\r\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-abvy7c9u\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-abvy7c9u\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4308 sha256=718ca1fa04765bd11dc70a03c138c094e221f1aeb1b6b6f2f5e208200cf96a9d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d_eupblg/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FdWKpayhPDvV",
        "outputId": "0fc49e48-6e63-4886-8f12-f5689e513c35"
      },
      "source": [
        "%%cuda --name matrix_multiplication.cu\r\n",
        "\r\n",
        "#include <stdio.h>\r\n",
        "#include <stdlib.h>\r\n",
        "#include <malloc.h>\r\n",
        "#include \"/content/sample_data/book.h\"\r\n",
        "#include<iostream>\r\n",
        "#include <cublas_v2.h>\r\n",
        "#include <time.h>\r\n",
        "#include \"cuda_runtime.h\"\r\n",
        "#include \"curand.h\"\r\n",
        "\r\n",
        "\r\n",
        "float* csv_to_matrix(int row_dim,int col_dim)\r\n",
        "{\r\n",
        "  float* mat=(float *)malloc(50000*512 *sizeof(float*));\r\n",
        "  FILE *open_file;\r\n",
        "\r\n",
        "\topen_file = fopen(\"Train_Features.txt\", \"r\");\r\n",
        "\r\n",
        "\tfor(int i = 0; i < row_dim; i++){\r\n",
        "\t\tfor (int j = 0; j < col_dim; j++)\r\n",
        "\t\tif (fscanf(open_file, \"%f,\", &mat[i * col_dim + j]) ==EOF) break;\r\n",
        "    //checking if the file is over or not\r\n",
        "\t\tif (feof(open_file)) break;\r\n",
        "\t} \r\n",
        "\tfclose(open_file);\r\n",
        "  return mat;\r\n",
        "}\r\n",
        "\r\n",
        "float random_values( float minimum, float maximum )\r\n",
        "{\r\n",
        "    float range = rand() / (float) RAND_MAX; \r\n",
        "    float value= minimum + range * ( maximum - minimum );\r\n",
        "    return value;   \r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "float* productcal(cublasHandle_t &handle,float* matrix1,float*matrix2,int row_matrix1,int col_matrix1,int row_matrix2, int col_matrix2){\r\n",
        "    float alpha=1.0;\r\n",
        "    float beta=0.0;   \r\n",
        "    float*result= (float*)malloc(row_matrix1*col_matrix2*sizeof(float));\r\n",
        "    float* dev_matrix1,*dev_matrix2,*dev_result;\r\n",
        "\r\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_matrix1, row_matrix1 *col_matrix1  * sizeof(float) ) );\r\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_matrix2, row_matrix2 * col_matrix2 *sizeof(float) ) );\r\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_result, row_matrix1 * col_matrix2 *sizeof(float) ) );\r\n",
        "\r\n",
        "    cudaMemcpy(dev_matrix1,matrix1,row_matrix1*col_matrix1*sizeof(float),cudaMemcpyHostToDevice);\r\n",
        "    cudaMemcpy(dev_matrix2,matrix2,row_matrix2*col_matrix2*sizeof(float),cudaMemcpyHostToDevice);\r\n",
        "\r\n",
        "    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, col_matrix2, row_matrix1,col_matrix1 , &alpha, dev_matrix2, col_matrix2, dev_matrix1, col_matrix1, &beta, dev_result, col_matrix2);\r\n",
        "\r\n",
        "    cudaMemcpy(result, dev_result, row_matrix1 * col_matrix2 *sizeof(float), cudaMemcpyDeviceToHost);\r\n",
        "    return result;\r\n",
        "\r\n",
        "    cudaFree(dev_matrix1);\r\n",
        "    cudaFree(dev_matrix2);\r\n",
        "    cudaFree(dev_result);\r\n",
        "    free(result);\r\n",
        "}\r\n",
        "\r\n",
        "float* add_bias(cublasHandle_t &handle,float* matrix1,float*matrix2,int row_matrix1,int col_matrix1,int row_matrix2, int col_matrix2){\r\n",
        "    float alpha=1.0;\r\n",
        "    float beta=1.0;   \r\n",
        "    float*result= (float*)malloc(row_matrix1*col_matrix2*sizeof(float));\r\n",
        "    float* dev_matrix1,*dev_matrix2,*dev_result;\r\n",
        "\r\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_matrix1, row_matrix1 *col_matrix1  * sizeof(float) ) );\r\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_matrix2, row_matrix2 * col_matrix2 *sizeof(float) ) );\r\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_result, row_matrix1 * col_matrix2 *sizeof(float) ) );\r\n",
        "\r\n",
        "    cudaMemcpy(dev_matrix1,matrix1,row_matrix1*col_matrix1*sizeof(float),cudaMemcpyHostToDevice);\r\n",
        "    cudaMemcpy(dev_matrix2,matrix2,row_matrix2*col_matrix2*sizeof(float),cudaMemcpyHostToDevice);\r\n",
        "\r\n",
        "    cublasSgeam(handle, CUBLAS_OP_N, CUBLAS_OP_N, row_matrix1, col_matrix1, &alpha, dev_matrix1, row_matrix1, &beta, dev_matrix2, row_matrix1, dev_result, row_matrix1);\r\n",
        "\r\n",
        "    cudaMemcpy(result, dev_result, row_matrix1 * col_matrix2 *sizeof(float), cudaMemcpyDeviceToHost);\r\n",
        "    return result;\r\n",
        "\r\n",
        "    cudaFree(dev_matrix1);\r\n",
        "    cudaFree(dev_matrix2);\r\n",
        "    cudaFree(dev_result);\r\n",
        "    free (result);\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "float* transpose(cublasHandle_t &handle,float* matrix1,int row_matrix1,int col_matrix1){\r\n",
        "    float alpha=1.0;\r\n",
        "    float beta=0.0;   \r\n",
        "    float*result= (float*)malloc(col_matrix1*row_matrix1*sizeof(float));\r\n",
        "    float* dev_matrix1,*dev_result;\r\n",
        "\r\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_matrix1, row_matrix1 *col_matrix1  * sizeof(float) ) );\r\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_result, col_matrix1 *row_matrix1  * sizeof(float) ) );\r\n",
        "    \r\n",
        "\r\n",
        "    cudaMemcpy(dev_matrix1,matrix1,row_matrix1*col_matrix1*sizeof(float),cudaMemcpyHostToDevice);\r\n",
        "\r\n",
        "    cublasSgeam(handle, CUBLAS_OP_T, CUBLAS_OP_N, row_matrix1, col_matrix1, &alpha, dev_matrix1, col_matrix1, &beta, dev_matrix1, row_matrix1, dev_result, row_matrix1);\r\n",
        "    cudaMemcpy(result, dev_result, col_matrix1 * row_matrix1 *sizeof(float), cudaMemcpyDeviceToHost);\r\n",
        "    return result;\r\n",
        "\r\n",
        "    cudaFree(dev_matrix1);\r\n",
        "    cudaFree(dev_result);\r\n",
        "    free (result);\r\n",
        "  }\r\n",
        "\r\n",
        "float * inverse(cublasHandle_t &Handle,float* m,int row_matrix1,int col_matrix1){\r\n",
        "    \r\n",
        "    float** array_dev_matrix1,** array_dev_resut;\r\n",
        "    float* dev_matrix1,* dev_result;\r\n",
        "    int* dev_matrix1_pivots,* dev_matrix1_info;\r\n",
        "\r\n",
        "    cudaMalloc(&array_dev_resut, row_matrix1 * col_matrix1 *sizeof(float*));\r\n",
        "    cudaMalloc(&array_dev_matrix1, row_matrix1 * col_matrix1 *sizeof(float*));\r\n",
        "    cudaMalloc(&dev_matrix1, row_matrix1 * col_matrix1 *sizeof(float*));\r\n",
        "    cudaMemcpy(dev_matrix1, m, row_matrix1 * col_matrix1 * sizeof(float*) , cudaMemcpyHostToDevice);\r\n",
        "    \r\n",
        "    cudaMalloc(&dev_result, row_matrix1 * col_matrix1 *sizeof(float*));    \r\n",
        "    cudaMemcpy(array_dev_matrix1, &dev_matrix1, sizeof(float*), cudaMemcpyHostToDevice);\r\n",
        "    cudaMalloc(&dev_matrix1_pivots, row_matrix1 * sizeof(int));\r\n",
        "    cudaMalloc(&dev_matrix1_info, sizeof(int));\r\n",
        "    \r\n",
        "    cudaMemcpy(array_dev_resut, &dev_result, sizeof(float*), cudaMemcpyHostToDevice);\r\n",
        "    \r\n",
        "    cublasSgetrfBatched(Handle, row_matrix1, array_dev_matrix1, row_matrix1, dev_matrix1_pivots, dev_matrix1_info, 1);\r\n",
        "    cudaDeviceSynchronize();\r\n",
        "\r\n",
        "    cublasSgetriBatched(Handle, row_matrix1, (const float **)array_dev_matrix1, row_matrix1, dev_matrix1_pivots, array_dev_resut, row_matrix1, dev_matrix1_info, 1);\r\n",
        "    cudaDeviceSynchronize();\r\n",
        "\r\n",
        "    float* result = (float*)malloc(row_matrix1 * col_matrix1 *sizeof(float*));\r\n",
        "    cudaMemcpy(result, dev_result, row_matrix1 * col_matrix1 *sizeof(float*), cudaMemcpyDeviceToHost);\r\n",
        "    \r\n",
        "    cudaFree(dev_matrix1_pivots);\r\n",
        "    cudaFree(dev_matrix1_info);\r\n",
        "    cudaFree(dev_matrix1);\r\n",
        "    cudaFree(dev_result);\r\n",
        "    cudaFree(array_dev_matrix1);\r\n",
        "    cudaFree(array_dev_resut);\r\n",
        "    \r\n",
        "    return result;\r\n",
        "    free(result);\r\n",
        "}\r\n",
        "\r\n",
        "float sigmoiddd(float x){\r\n",
        "     //float y = x / (1 + abs(x));\r\n",
        "     float y=max(0.0,x);\r\n",
        "     return y;\r\n",
        "    \r\n",
        "}\r\n",
        "\r\n",
        "int main(){\r\n",
        "  \r\n",
        "  int hidden_neurons=2500;\r\n",
        "  int weight_x=512;\r\n",
        "  int weight_y=hidden_neurons;\r\n",
        "  int bias_x=50000;\r\n",
        "  int bias_y=hidden_neurons;\r\n",
        "  float rand_array[bias_y]; \r\n",
        "  \r\n",
        "  float* mat=(float *)malloc(50000*512 *sizeof(float*));\r\n",
        "  float* test_features_mat=(float *)malloc(10000*512 *sizeof(float*));\r\n",
        "\r\n",
        "  mat= csv_to_matrix(50000,512);  \r\n",
        "\r\n",
        "  float *weights = (float *)malloc(weight_x *weight_y  * sizeof(float));\r\n",
        "  float *bias = (float *)malloc(bias_x * bias_y * sizeof(float));\r\n",
        "  float * H=(float *)malloc(bias_x * bias_y * sizeof(float));\r\n",
        "  float * H_transpose=(float *)malloc(hidden_neurons * 50000 * sizeof(float));\r\n",
        "  float * H_transpose_H=(float *)malloc(hidden_neurons * hidden_neurons * sizeof(float));\r\n",
        "  float *H_inverse = (float *)malloc(hidden_neurons * hidden_neurons * sizeof(float));\r\n",
        "  float *pseudo_inverse=(float*)malloc(hidden_neurons * 50000 * sizeof(float));\r\n",
        "  float *Beta=(float*)malloc(hidden_neurons * 100 * sizeof(float));\r\n",
        "  float* train_label=(float*)malloc(50000*100 *sizeof(float*));\r\n",
        "  float* output=(float*)malloc(50000*100 *sizeof(float*));\r\n",
        "\r\n",
        "\r\n",
        "   for(int i=0;i<weight_x;i++){\r\n",
        "        for(int j=0;j<weight_y;j++){\r\n",
        "          *(weights + i*weight_y + j)=random_values(-1.0,1.0);  \r\n",
        "          //printf(\"%f \\t\",*(weights + i*weight_y + j));\r\n",
        "         }\r\n",
        "      }\r\n",
        "   \r\n",
        "   for (int i=0;i<bias_y;i++){\r\n",
        "       rand_array[i]=random_values(-1.0,1.0);\r\n",
        "   }\r\n",
        "   \r\n",
        "   for(int i=0;i<bias_x;i++){\r\n",
        "        for(int j=0;j<bias_y;j++){\r\n",
        "          *(bias + i*bias_y + j)=rand_array[j];  \r\n",
        "         }\r\n",
        "      }\r\n",
        "\r\n",
        "  cublasHandle_t handle;\r\n",
        "  cublasCreate(&handle);\r\n",
        "\r\n",
        "  float *weights_features=productcal(handle,mat,weights,50000,512,weight_x,weight_y);\r\n",
        "  float *bias_add=add_bias(handle,weights_features,bias,50000,hidden_neurons,bias_x,bias_y);\r\n",
        "  \r\n",
        "  for(int i=0;i<bias_x;i++){\r\n",
        "        for(int j=0;j<bias_y;j++){\r\n",
        "          *(H + i*bias_y + j)=sigmoiddd(*(bias_add+i*bias_y  + j));  \r\n",
        "         }\r\n",
        "         }\r\n",
        "\r\n",
        "   free(bias_add);\r\n",
        "   free(weights_features);      \r\n",
        "   \r\n",
        "   // Process of calculating Pseudo Inverse Of H.\r\n",
        "   \r\n",
        "   //First step : Computing Transpose of H and its matrix_multiplication with its normal form. \r\n",
        "  \r\n",
        "   H_transpose= transpose(handle,H,50000,hidden_neurons);\r\n",
        "   H_transpose_H= productcal(handle,H_transpose,H,hidden_neurons,50000,50000,hidden_neurons);\r\n",
        "   H_inverse=inverse(handle,H_transpose_H,hidden_neurons,hidden_neurons);\r\n",
        "   free(H_transpose_H);\r\n",
        "   pseudo_inverse= productcal(handle,H_inverse,H_transpose,hidden_neurons,hidden_neurons,hidden_neurons,50000);\r\n",
        "   \r\n",
        "  free(H_transpose);  \r\n",
        "  \r\n",
        "  FILE *open_file2;\r\n",
        "\r\n",
        "\topen_file2 = fopen(\"Train_labels.txt\", \"r\");\r\n",
        "\r\n",
        "\tfor(int i = 0; i < 50000; i++){\r\n",
        "\t\tfor (int j = 0; j < 100; j++)\r\n",
        "\t\tif (fscanf(open_file2, \"%f,\", &train_label[i * 100 + j]) ==EOF) break;\r\n",
        "    //checking if the file is over or not\r\n",
        "\t\tif (feof(open_file2)) break;\r\n",
        "\t} \r\n",
        "\tfclose(open_file2);\r\n",
        "\r\n",
        "  Beta=productcal(handle,pseudo_inverse,train_label,hidden_neurons,50000,50000,100);\r\n",
        "  free(pseudo_inverse);\r\n",
        "  output= productcal(handle,H,Beta,50000,hidden_neurons,hidden_neurons,100);\r\n",
        "\r\n",
        "FILE *open_file3;\r\n",
        "\r\n",
        "open_file3 = fopen(\"output_train.csv\", \"w+\"); \r\n",
        "\r\n",
        " \r\n",
        "for(int i=0;i<50000;i++)\r\n",
        "{\r\n",
        " \r\n",
        "    for(int j=0;j<100;j++)\r\n",
        " {      \r\n",
        "        fprintf(open_file3,\",%f \",* (output+ i*100+ j));\r\n",
        "\r\n",
        "        }\r\n",
        "            fprintf(open_file3,\"\\n\");\r\n",
        "}\r\n",
        "fclose(open_file3);\r\n",
        "\r\n",
        "free(output);\r\n",
        "FILE *open_file4;\r\n",
        "\r\n",
        "\topen_file4 = fopen(\"Testing_features.txt\", \"r\");\r\n",
        "\r\n",
        "\tfor(int i = 0; i < 10000; i++){\r\n",
        "\t\tfor (int j = 0; j < 512; j++)\r\n",
        "\t\tif (fscanf(open_file4, \"%f,\", &test_features_mat[i * 512 + j]) ==EOF) break;\r\n",
        "    //checking if the file is over or not\r\n",
        "\t\tif (feof(open_file4)) break;\r\n",
        "\t} \r\n",
        "   \r\n",
        "\tfclose(open_file4);\r\n",
        "  free(H);\r\n",
        "   weights_features=productcal(handle,test_features_mat,weights,10000,512,weight_x,weight_y);\r\n",
        "   bias_add=add_bias(handle,weights_features,bias,10000,hidden_neurons,10000,bias_y);\r\n",
        "   float *H_test= (float *)malloc(10000 * bias_y * sizeof(float));\r\n",
        "   for(int i=0;i<10000;i++){\r\n",
        "    for(int j=0;j<bias_y;j++){\r\n",
        "      *(H_test + i*bias_y + j)=sigmoiddd(*(bias_add+i*bias_y  + j));  \r\n",
        "         }\r\n",
        "\r\n",
        "         }\r\n",
        "  float*test_output=productcal(handle,H_test,Beta,10000,hidden_neurons,hidden_neurons,100);\r\n",
        "\r\n",
        "  FILE *open_file5;\r\n",
        "\r\n",
        "  open_file5 = fopen(\"output_test.csv\", \"w+\"); \r\n",
        "\r\n",
        " \r\n",
        "  for(int i=0;i<10000;i++)\r\n",
        "  {\r\n",
        " \r\n",
        "    for(int j=0;j<100;j++)\r\n",
        "  {      \r\n",
        "        fprintf(open_file5,\",%f \",* (test_output+ i*100+ j));\r\n",
        "\r\n",
        "        }\r\n",
        "            fprintf(open_file5,\"\\n\");\r\n",
        "  }\r\n",
        "  fclose(open_file5);\r\n",
        "\r\n",
        "  free(test_output);\r\n",
        " \r\n",
        "}    \r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/matrix_multiplication.cu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTMkXxv-QnH7"
      },
      "source": [
        "!nvcc -o /content/src/matrix_multiplication /content/src/matrix_multiplication.cu -lcublas -lcurand"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIwFvN0mQpi9"
      },
      "source": [
        "!/content/src/matrix_multiplication"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax_FdLGbQrIE"
      },
      "source": [
        "\r\n",
        "#np.loadtxt(open(\"output_train.csv\", \"rb\"), delimiter=\",\",skiprows=0)  \r\n",
        "predicted=pd.read_csv(\"output_train.csv\")\r\n",
        "\r\n",
        "#predicted_labels=predicted[:,1:]\r\n",
        "#predicted.set_axis(['C1','c2','c3','c4','c5','c6','c7','c8','c9','c10','c11'], axis='columns', inplace=False)\r\n",
        "predicted.drop(predicted.columns[0], axis=1, inplace=True)\r\n",
        "row1=predicted.columns\r\n",
        "predicted.loc[-1] = row1  # adding a row\r\n",
        "predicted.index = predicted.index + 1  # shifting index\r\n",
        "predicted.sort_index(inplace=True)\r\n",
        "\r\n",
        "predicted=predicted.to_numpy()\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J-BWryRQxTb"
      },
      "source": [
        "def accuracy(Error,target):\r\n",
        "    count=0\r\n",
        " \r\n",
        "    for i in range(len(Error)):\r\n",
        "      index=np.argmax(target[i])    # getting the index of the target label\r\n",
        "      index_predicted=np.argmax(Error[i]) #getting the index of the predicted label \r\n",
        "      if index_predicted==index:\r\n",
        "        count=count+1\r\n",
        "    predicted_accuracy=(count/len(Error))*100   #calculating the accuracy based on the total correct predictions vs total values\r\n",
        "    #print(predicted_accuracy)\r\n",
        "    return predicted_accuracy"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gwmq79hvQyCl",
        "outputId": "7f4591a6-a85e-45dc-ac77-b50159d033ad"
      },
      "source": [
        "predicted_accuracy=accuracy(predicted,train_label) #Calculating the accuracy of the predicted labels\r\n",
        "print(predicted_accuracy)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62.57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXuojBV3Qztx"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "predicted_test=pd.read_csv(\"output_test.csv\")\r\n",
        "\r\n",
        "#predicted_labels=predicted[:,1:]\r\n",
        "#predicted.set_axis(['C1','c2','c3','c4','c5','c6','c7','c8','c9','c10','c11'], axis='columns', inplace=False)\r\n",
        "predicted_test.drop(predicted_test.columns[0], axis=1, inplace=True)\r\n",
        "row1_test=predicted_test.columns\r\n",
        "predicted_test.loc[-1] = row1_test  # adding a row\r\n",
        "predicted_test.index = predicted_test.index + 1  # shifting index\r\n",
        "predicted_test.sort_index(inplace=True)\r\n",
        "predicted_test=predicted_test.to_numpy()\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70xdIvD9Q4wo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c66d1e0-1981-4feb-aa0a-55a89f959c64"
      },
      "source": [
        "predicted_test_accuracy=accuracy(predicted_test,test_label) #Calculating the accuracy of the predicted labels\r\n",
        "print(predicted_test_accuracy)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj9S1a0mQ6PX"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}